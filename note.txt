x. training的时候step embedding只到5，推理的时候设置成10然后load ckpt，理论上会存在embedding形状对不齐的问题
x. 如果用attention mask+iterative，是不是非常省资源但是效果拔群？
x. 和recursive、扩展epoch数对比
x. 当前测试脚本，定义模型的时候会规定num_iterations，之前step_embedding的做法无法在num_iteration不对齐的情况下推理，无论是大是小。所以回退到之前的版本。