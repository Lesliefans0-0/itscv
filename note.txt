# Research Notes - Inference-Time-Scaling Computer Vision

## Technical Challenges and Observations

1. **Training vs Inference Token Mismatch**: When training with step embedding limited to 5 iterations but inference set to 10, there's a potential embedding shape misalignment issue.

2. **Attention Mask + Iterative Approach**: Investigating whether combining attention masks with iterative processing could be more resource-efficient while maintaining performance.

3. **Model Comparison**: Need to compare recursive approaches with extended epoch training to understand trade-offs.

4. **Inference Flexibility**: Current test scripts define model with fixed num_iterations, making it difficult to use step_embedding approach when training and inference iteration counts don't match.

## Experiment Results

### vit_iterative_2025-05-12-2352 and vit_iterative_2025-05-12-2356
- **Version 0**: Based on step token approach - showed poor performance
- **Version 1**: Based on step embedding - no improvement, still has training stability issues
- **Hypothesis**: Issues may not be related to token/embedding selection, but rather loss propagation method
- **Previous approach**: Only propagated loss from final iteration
- **Current approach**: Investigating whether loss propagation method is the core issue or if warmup is needed

## Key Research Questions

1. Is the fundamental issue related to loss propagation method rather than token/embedding selection?
2. Would proper warmup scheduling improve training stability?
3. Can attention masking provide better resource efficiency for iterative approaches?
4. How do recursive approaches compare to extended training epochs?

## Next Steps

- Implement proper warmup scheduling
- Test attention mask integration
- Compare recursive vs iterative approaches
- Investigate loss propagation strategies

